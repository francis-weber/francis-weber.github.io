# Batch Processing

One of the things fastai has taught me is about the impact of different batch sizes on processing speed and GPU performance. If used correctly, I can drastically speed up the inference time of a computer vision model.

## How it Works

Batch processing in computer vision splits training data into groups to process multiple images simultaneously through the nueral network. This leverages GPU parallelisation, improving training time without negatively impacting the quality of the model.

In fastai, this can be simply modified in a datablock section with:

```python
# Example of setting batch size to 128
bs=128
```

In messing around with these values, I have found that increasing batch size doenst always result in a faster training. Evaluationg the size of your initial dataset and the available GPU usage is important to find the optimal size paramater.

In my own free time, I've had experience running YOLO models on basketball footage for player detection:

![](/images/BBallYolo.png "Example of Basketball YOLO detection")

One thing I have struggled with in the past with this, processing a long video can take a lot of time to do locally on my machine. Now I know about batch processing more, I can work out what the most efficient batch size is, run on Google Colab, and save a bunch of time in my basketball computer vision projects. 
